{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means聚类器各参数含义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.cluster.KMeans(n_clusters=8,\n",
    "    init='k-means++', \n",
    "\tn_init=10, \n",
    "\tmax_iter=300, \n",
    "\ttol=0.0001, \n",
    "\tprecompute_distances='auto', \n",
    "\tverbose=0, \n",
    "\trandom_state=None, \n",
    "\tcopy_x=True, \n",
    "\tn_jobs=1, \n",
    "\talgorithm='auto'\n",
    "\t)\n",
    "```\n",
    "1. n_clusters:簇的个数，即你想聚成几类\n",
    "2. init: 初始簇中心的获取方法\n",
    "3. n_init: 获取初始簇中心的更迭次数，为了弥补初始质心的影响，算法默认会初始10次质心，实现算法，然后返回最好的结果。\n",
    "4. max_iter: 最大迭代次数（因为kmeans算法的实现需要迭代）\n",
    "5. tol: 容忍度，即kmeans运行准则收敛的条件\n",
    "6. precompute_distances：是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True 会把整个距离矩阵都放到内存中，auto 会默认在数据样本大于featurs*samples 的数量大于12e6 的时候False,False 时核心实现的方法是利用Cpython 来实现的\n",
    "7. verbose: 冗长模式（不太懂是啥意思，反正一般不去改默认值）\n",
    "8. random_state: 随机生成簇中心的状态条件。\n",
    "9. copy_x: 对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool 在scikit-learn 很多接口中都会有这个参数的，就是是否对输入数据继续copy 操作，以便不修改用户的输入数据。这个要理解Python 的内存机制才会比较清楚。\n",
    "10. n_jobs: 使用进程的数量\n",
    "11. algorithm: kmeans的实现算法，有：‘auto’, ‘full’, ‘elkan’, 其中 'full’表示用EM方式实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means例子（划分聚类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 1, 2, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 0, 2, 0,\n",
       "        0, 0, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 0, 2, 0, 0, 1, 2, 1, 2, 2,\n",
       "        1, 1, 2, 2, 1, 2, 0, 1, 2, 0, 2, 2, 0, 1, 1, 1, 2, 1, 2, 2, 1, 2,\n",
       "        1, 2, 2, 0, 2, 2, 2, 0, 2, 0, 0, 2, 1, 1, 0, 1, 2, 2, 1, 1, 1, 2,\n",
       "        2, 2, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2]),\n",
       " array([[0.46540716, 0.24203929, 0.84406913],\n",
       "        [0.41166667, 0.26790177, 0.29887887],\n",
       "        [0.48311733, 0.78749247, 0.50603071]]),\n",
       " 13.41782916898357)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "data = np.random.rand(100, 3) #生成一个随机数据，样本大小为100, 特征数为3\n",
    "\n",
    "#假如我要构造一个聚类数为3的聚类器\n",
    "estimator = KMeans(n_clusters=3)#构造聚类器\n",
    "estimator.fit(data)#聚类(训练)\n",
    "label_pred = estimator.labels_ #获取聚类标签\n",
    "centroids = estimator.cluster_centers_ #获取聚类中心\n",
    "inertia = estimator.inertia_ # 获取聚类准则的总和（平方差）\n",
    "label_pred,centroids,inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN（密度聚类）（去噪声）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN聚类器各参数含义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. eps： DBSCAN算法参数，即我们的ϵ-邻域的距离阈值，和样本距离超过ϵ的样本点不在ϵ-邻域内。默认值是0.5.一般需要通过在多组值里面选择一个合适的阈值。eps过大，则更多的点会落在核心对象的ϵ-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。\n",
    "\n",
    "2. min_samples： DBSCAN算法参数，即样本点要成为核心对象所需要的ϵ-邻域的样本数阈值。默认值是5. 一般需要通过在多组值里面选择一个合适的阈值。通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。\n",
    "\n",
    "3. metric：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。\n",
    "4. algorithm：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现， ‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。\n",
    "5. leaf_size：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。\n",
    "6. p: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "X1,y1 = datasets.make_circles(n_samples = 5000,factor = 0.6,noise = 0.05)  # factor:外圈与内圈的尺度因子\n",
    "dbscan = DBSCAN(eps=10, min_samples=1)\n",
    "cluster = dbscan.fit_predict(X1)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCSCAN与k-means对比分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。\n",
    "\n",
    "　　　　那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。\n",
    "\n",
    "　　　　下面对DBSCAN算法的优缺点做一个总结。\n",
    "\n",
    "　　　　DBSCAN的主要优点有：\n",
    "\n",
    "　　　　1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。\n",
    "\n",
    "　　　　2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。\n",
    "\n",
    "　　　　3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。\n",
    "\n",
    "　　　　DBSCAN的主要缺点有：\n",
    "\n",
    "　　　　1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。\n",
    "\n",
    "　　　　2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。\n",
    "\n",
    "　　　　3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGNES（层次聚类）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGNES聚类器各参数含义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class sklearn.cluster.AgglomerativeClustering\n",
    "    (n_clusters=2, affinity='euclidean', memory=None, \n",
    "    connectivity=None, compute_full_tree='auto', linkage='ward', \n",
    "    pooling_func=<function mean at 0x174b938>)\n",
    "```\n",
    "1. n_clusters：目标类别数，默认是2。\n",
    "2. affinity：样本点之间距离计算方式，可以是euclidean(欧式距离), l1、 l2、manhattan(曼哈顿距离)、cosine(余弦距离)、precomputed(可以预先设定好距离)，如果参数linkage选择“ward”的时候只能使用euclidean。\n",
    "3. linkage：链接标准，即样本点的合并标准，主要有ward、complete、average三个参数可选，默认是ward。每个簇（类）本身就是一个集合，我们在合并两个簇的时候其实是在合并两个集合，所以我们需要找到一种计算两个集合之间距离的方式，主要有这三种方式：ward、complete、average，分别表示使用两个集合方差、两个集合中点与点距离之间的平均值、两个集合中距离最小的两个点的距离\n",
    "\n",
    "##### 对象/属性\n",
    "* labels_:每个样本点的类别。\n",
    "* n_leaves_:层次树中叶结点树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 层次聚类例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 2, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0,\n",
       "       2, 1, 1, 0, 2, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 2, 0, 1,\n",
       "       0, 0, 0, 0, 1, 2, 0, 1, 1, 2, 0, 1, 2, 0, 1, 2, 2, 2, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 2, 2, 2, 1, 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering as ag\n",
    "import numpy as np\n",
    "Data = np.random.rand(100, 3) #生成一个随机数据，样本大小为100, 特征数为3\n",
    "agnes = ag(n_clusters=3)#构造聚类器\n",
    "agnes.fit(Data)#聚类(训练)\n",
    "agnes.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 层次聚类优缺点\n",
    "* 优点：1，距离和规则的相似度容易定义，限制少；2，不需要预先制定聚类数；3，可以发现类的层次关系；4，可以聚类成其它形状\n",
    "\n",
    "* 缺点：1，计算复杂度太高；2，奇异值也能产生很大影响；3，算法很可能聚类成链状"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
