#### 机器学习模型的训练除了考虑模型的准确性之外，还要考虑计算性能。
#### 冗余的特征虽然不会影响到模型的性能，但却会使得计算机做出无用功。
#### 特征筛选与PCA这类通过选择主成分对特征进行重建的方法有所区别：PCA压缩之后的特征往往无法解释；但是特征筛选不存在对特征值的修改，而更加侧重于寻找那些对模型的性能提升较大的少量特征。
```
##此模型采用的titanic的数据集
import pandas as pd
titanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')
##分离数据特征与预测目标
y = titanic['survived']
X = titanic.drop(['row.names','name','survived'],axis=1)
##对缺失数据进行填充
X['age'].fillna(X['age'].mean(),inplace=True)
X.fillna('UNKNOWN',inplace=True)
##分割数据,依然采用25%用于测试
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=33)
##类别型特征向量
from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()
X_train = vec.fit_transform(X_train.to_dict(orient='record'))
X_test = vec.transform(X_test.to_dict(orient='record'))  
##输出处理后特征向量的维度
print(len(vec.feature_names_))
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190713153851256.png)
```
##使用决策树模型依靠所有特征进行预测,并作出性能评价
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(criterion='entropy')
dt.fit(X_train,y_train)
dt.score(X_test,y_test)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190713153915326.png)
```
from sklearn import feature_selection
#筛选前20%的特征,使用相同配置的决策树模型进行预测,并且评估性能
fs = feature_selection.SelectPercentile(feature_selection.chi2,percentile=20)
X_train_fs = fs.fit_transform(X_train,y_train)
dt.fit(X_train_fs,y_train)
X_test_fs = fs.transform(X_test)
dt.score(X_test_fs,y_test)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190713153945777.png)
```
##通过交叉验证的方法,按照固定间隔的百分比筛选特征,并作图展示性能随
##特征筛选比例的变化
from sklearn.cross_validation import cross_val_score
import numpy as np
percentiles = range(1,100,2)
results = []

for i in percentiles:
    fs = feature_selection.SelectPercentile(feature_selection.chi2,percentile=i)
    X_train_fs = fs.fit_transform(X_train,y_train)
    scores = cross_val_score(dt,X_train_fs,y_train,cv=5)
    results = np.append(results,scores.mean())
print(results)
##找到体现最佳性能特征筛选的百分比
opt = np.where(results == results.max())[0][0]
print('Optimal number of features %d' %percentiles[opt])
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190713154026327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dwcm9mZXNzb3I=,size_16,color_FFFFFF,t_70)
```
#分布的可视化
import pylab as pl
pl.plot(percentiles,results)
pl.xlabel('percentiles of features')
pl.ylabel('accuracy')
pl.show()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190713154046461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dwcm9mZXNzb3I=,size_16,color_FFFFFF,t_70)
```
##使用最佳筛选后的特征，利用相同配置的模型在测试集上进行性能评估
from sklearn import feature_selection
fs = feature_selection.SelectPercentile(feature_selection.chi2,percentile=7)
X_train_fs = fs.fit_transform(X_train,y_train)
dt.fit(X_train_fs,y_train)
X_test_fs = fs.transform(X_test)
dt.score(X_test_fs,y_test)
```
通过以上的代码结果与输出，我们可以得到如下的结果：

（1)： 初步筛选后，最终的训练与测试数据集有474个维度的特征值

（2)： 直接用474个维度进行决策树进行预测，测试集上的表现为81.76%

（3)： 如果筛选前20%的特征，测试集上的性能表现为82.37%

（4)： 如果按照固定的间隔采用不同的百分比的特征进行训练与测试，可以看到最好的训练在前7%左右

（5)； 使用前7%的数据集进行预测，该分类器的性能达到了85.71%，比预测全部的维度提高了四个百分点

参考文章：https://zhuanlan.zhihu.com/p/34940911